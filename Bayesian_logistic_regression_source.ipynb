{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16312864",
   "metadata": {},
   "source": [
    "### Logistic Regression (pytorch)\n",
    "\n",
    "Machine Learning Algorithms 2024\n",
    "\n",
    "Dates: 2024-3-9\n",
    "\n",
    "Author: Yung-Kyun Noh\n",
    "\n",
    "Department of Computer Science, Hanyang University & School of Computational Sciences, KIAS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b774b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usage example:\n",
    "# getLogGaussian_torch(data1, torch.tensor(mean1).to('cuda'), torch.tensor(Cov1).to('cuda'))\n",
    "# getLogGaussian_torch(data1[0].reshape([1,-1]), torch.tensor(mean1).to('cuda'), torch.tensor(Cov1).to('cuda'))\n",
    "def getLogGaussian_torch(X, mu, Sig):\n",
    "    ''' evalute log probability of Gaussian distribution\n",
    "    given the Gaussian distribution model (mean and covariance).'''\n",
    "    # X: datanum x dim\n",
    "    nData, nDim = X.shape\n",
    "    w,v = torch.linalg.eigh(Sig)  # w: eigenvalues\n",
    "    log_det_Sig = torch.sum(torch.log(w))\n",
    "    invSig = torch.matmul(torch.matmul(v, torch.diag(1/w)), v.T)\n",
    "    logPs = torch.zeros(nData)\n",
    "\n",
    "    const = - nDim / (2.) * torch.log(torch.tensor(2*torch.pi)) \\\n",
    "        - 1 / (2.) * log_det_Sig # sacalar\n",
    "\n",
    "    X_sub_m = X - mu\n",
    "    expvals = - 1/2. * torch.sum(torch.matmul(X_sub_m, invSig)*X_sub_m, 1)\n",
    "    logPs =  const + expvals\n",
    "    return logPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811343db",
   "metadata": {},
   "source": [
    "### Generate two Gaussian densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "datanum1 = 100\n",
    "datanum2 = 100\n",
    "m1 = torch.tensor([-1,0], dtype=torch.float32)\n",
    "Sig1 = torch.tensor([[2,.5],[.5,2]], dtype=torch.float32)\n",
    "m2 = torch.tensor([1,0], dtype=torch.float32)\n",
    "Sig2 = torch.tensor([[1,0],[0,.5]], dtype=torch.float32)\n",
    "\n",
    "L = torch.linalg.cholesky(Sig1)\n",
    "data1 = torch.matmul(torch.randn([datanum1,dim], device=device), L.to(device).T) + m1.to(device)\n",
    "L = torch.linalg.cholesky(Sig2)\n",
    "data2 = torch.matmul(torch.randn([datanum2,dim], device=device), L.to(device).T) + m2.to(device)\n",
    "\n",
    "all_data = torch.cat([data1, data2])\n",
    "ys = torch.cat([1*torch.ones(datanum1), -1*torch.ones(datanum2)]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data1[:,0].to('cpu'), data1[:,1].to('cpu'))\n",
    "plt.scatter(data2[:,0].to('cpu'), data2[:,1].to('cpu'))\n",
    "xmin, xmax, ymin, ymax = plt.axis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec7ab4",
   "metadata": {},
   "source": [
    "### Draw an optimal boundary for two Gaussians\n",
    "\n",
    "Optimal boundary and the true class-posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_resolution = 100\n",
    "X = torch.arange(xmin, xmax, (xmax - xmin)/(grid_resolution - 0.01))\n",
    "Y = torch.arange(ymin, ymax, (ymax - ymin)/(grid_resolution - 0.01))\n",
    "\n",
    "grid_x, grid_y = torch.meshgrid(X, Y, indexing='ij')\n",
    "scan_inputs = torch.cat([grid_x.reshape([-1,1]), grid_y.reshape([-1,1])], axis=1)\n",
    "discri_vals = getLogGaussian_torch(scan_inputs, m1, Sig1) - getLogGaussian_torch(scan_inputs, m2, Sig2)\n",
    "discri_vals = discri_vals.reshape([grid_resolution,grid_resolution]).T\n",
    "plt.contour(X, Y, discri_vals, [0])\n",
    "plt.scatter(data1[:,0].to('cpu'), data1[:,1].to('cpu'))\n",
    "plt.scatter(data2[:,0].to('cpu'), data2[:,1].to('cpu'))\n",
    "plt.title('Optimal Boundary')\n",
    "plt.show()\n",
    "\n",
    "plt.contourf(X, Y, 1/(1 + torch.exp(-discri_vals)))\n",
    "plt.colorbar()\n",
    "plt.scatter(data1[:,0].to('cpu'), data1[:,1].to('cpu'))\n",
    "plt.scatter(data2[:,0].to('cpu'), data2[:,1].to('cpu'))\n",
    "plt.title('True Posteriors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f111899",
   "metadata": {},
   "source": [
    "### Make a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "     def forward(self, x):\n",
    "         outputs = torch.sigmoid(self.linear(x))\n",
    "         return outputs\n",
    "\n",
    "model = LogisticRegression(input_dim=dim, output_dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12236f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plot(model,X,y,title):\n",
    "    parm = {}\n",
    "    b = []\n",
    "    for name, param in model.named_parameters():\n",
    "        parm[name]=param.detach().to('cpu').numpy()  \n",
    "    \n",
    "    w = parm['linear.weight'][0]\n",
    "    b = -parm['linear.bias'][0]\n",
    "    print(w, b)\n",
    "    plt.scatter(X[:, 0].to('cpu'), X[:, 1].to('cpu'), c=y.to('cpu'), cmap='jet')\n",
    "    u = np.linspace(X[:, 0].min().to('cpu'), X[:, 0].max().to('cpu'), 2)\n",
    "    plt.plot(u, (b-w[0]*u)/w[1])\n",
    "    plt.xlim(X[:, 0].min().to('cpu')-0.5, X[:, 0].max().to('cpu')+0.5)\n",
    "    plt.ylim(X[:, 1].min().to('cpu')-0.5, X[:, 1].max().to('cpu')+0.5)\n",
    "    plt.xlabel('$x_1$',fontsize=16) # Normally you can just add the argument fontweight='bold' but it does not work with latex\n",
    "    plt.ylabel('$x_2$',fontsize=16)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_plot(ws, bs, X, y, title):\n",
    "    for w,b in zip(ws, bs):\n",
    "#         print(w, b)\n",
    "        u = np.linspace(X[:, 0].min(), X[:, 0].max(), 2)\n",
    "        plt.plot(u, (b-w[0]*u)/w[1], color='red', zorder=1)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', zorder=2)\n",
    "    plt.xlim(X[:, 0].min()-0.5, X[:, 0].max()+0.5)\n",
    "    plt.ylim(X[:, 1].min()-0.5, X[:, 1].max()+0.5)\n",
    "    plt.xlabel('$x_1$',fontsize=16) # Normally you can just add the argument fontweight='bold' but it does not work with latex\n",
    "    plt.ylabel('$x_2$',fontsize=16)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1a402",
   "metadata": {},
   "source": [
    "### Consider a Monte Carlo approximation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[f(\\mathbf{x})] &=& \\int p(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x} \\\\\n",
    "&=& \\sum_{\\mathbf{x}\\sim p(\\mathbf{x})} f(\\mathbf{x})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w, b sampling\n",
    "n_param_sample = 30000\n",
    "\n",
    "w_param_var = 1\n",
    "b_param_var = 30\n",
    "ws = np.sqrt(w_param_var)*torch.randn([n_param_sample, dim], device=device)\n",
    "bs = np.sqrt(w_param_var)*np.sqrt(b_param_var)*torch.randn([n_param_sample, 1], device=device)\n",
    "\n",
    "max_plot_boundaries = 300\n",
    "models_plot(ws[0:np.min([n_param_sample, max_plot_boundaries])].to('cpu'), \\\n",
    "            bs[0:np.min([n_param_sample, max_plot_boundaries])].to('cpu'), all_data.to('cpu'), ys.to('cpu'), \\\n",
    "            'Data and sample parameters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889adc33",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "\n",
    "We want to calculate\n",
    "\\begin{eqnarray}\n",
    "P(y|\\bx, Y, X) &=& \\int P(y|\\bx, \\theta)p(\\theta|Y,X) d\\theta \\\\\n",
    "&=& \\int P(y|\\bx, \\theta) \\frac{P(Y|X,\\theta)p(\\theta)}{P(Y|X)} d\\theta \\\\\n",
    "&=& \\int P(y|\\bx, \\theta) \\frac{p(\\theta)\\prod_{i = 1}^N P(y_i|\\bx_i,\\theta)}{\\int p(\\theta)\\prod_{i = 1}^N P(y_i|\\bx_i,\\theta)d\\theta} d\\theta\n",
    "\\end{eqnarray}\n",
    "with $\\theta = \\{\\bw, b\\}$, $y\\in\\{1,-1\\}$.\n",
    "\n",
    "There are two ways of sampling: one way is to sample data from $p(\\theta|Y,X)$ and the other is to sample from $p(\\theta)$. \n",
    "\n",
    "When we sample data from $p(\\theta|Y,X)$, we can use the following equation:\n",
    "\\begin{eqnarray}\n",
    "P(y=1|\\bx, Y,X) &=& \\int P(y=1|\\bx, \\theta) p(\\theta|Y,X) d\\theta \\\\\n",
    "&\\approx& \\frac{1}{M} \\sum_{j = 1}^M \\frac{1}{1 + \\exp(-\\bw_j^\\top \\bx + b_j)}\n",
    "\\end{eqnarray}\n",
    "using $M$ number of samples from $\\theta_j \\sim p(\\theta|Y,X)$.\n",
    "\n",
    "When we sample from $p(\\theta)$, we can consider using the following equation:\n",
    "\\begin{eqnarray}\n",
    "P(y=1|\\bx, Y,X) &=& \\int P(y=1|\\bx, \\theta) p(\\theta|Y,X) d\\theta \\\\\n",
    "&=& \\int P(y=1|\\bx, \\theta) \\frac{p(\\theta)\\prod_{i = 1}^N P(y_i|\\bx_i,\\theta)}{\\int p(\\theta)\\prod_{i = 1}^N P(y_i|\\bx_i,\\theta)d\\theta} d\\theta \\\\\n",
    "&\\approx& \\frac{1}{M} \\sum_{j = 1}^M \\frac{1}{1 + \\exp(-\\bw_j^\\top \\bx + b_j)}\\cdot\n",
    "\\frac{1}{\\frac{1}{M}\\sum_{k = 1}^M \\exp\\left[\\sum_{i = 1}^N \\log(1 + \\exp(-y_i(\\bw_j^\\top\\bx_i - b_j))) - \\log(1 + \\exp(-y_i(\\bw_k^\\top\\bx_i - b_k))) \\right]}\n",
    "\\end{eqnarray}\n",
    "using $M$ number of samples from $\\theta_j \\sim p(\\theta)$. Here, we used the following equality:\n",
    "\\begin{eqnarray}\n",
    "\\Big( P(Y|X) = \\Big)  \\prod_{i = 1}^N P(y_i|\\bx_i, \\theta_j) &=& \\prod_{i = 1}^N \\frac{1}{1 + \\exp(-y_i(\\bw_j^\\top\\bx_i - b_j))} \\\\\n",
    "&=& \\exp\\left[-\\sum_{i = 1}^N \\log\\left(1 + \\exp(-y_i(\\bw_j^\\top\\bx_i - b_j))\\right)\\right].\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f34676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC sample x datanum\n",
    "a_vals = (torch.matmul(ws, all_data.T) - bs)*ys  # a = y(w'x-b)\n",
    "log_1_exp_na = torch.log(1 + torch.exp(-a_vals))  # log(1 + exp(-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439c3a2",
   "metadata": {},
   "source": [
    "### Some parameters have large posteriors and others do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_1_exp_na_datasum = torch.sum(log_1_exp_na, axis=1)\n",
    "Posteriors = 1/torch.sum(torch.exp(log_1_exp_na_datasum.reshape([1,-1]) - \\\n",
    "                                   log_1_exp_na_datasum.reshape([-1,1])), axis=0)\n",
    "\n",
    "# Posterior spectrum\n",
    "plt.plot(torch.sort(Posteriors, descending=True).values[0:1000].to('cpu'), 'o-') # Some largest posteriors\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24604df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Posteriors[torch.argsort(Posteriors, descending=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Posteriors should be sum to one\n",
    "torch.sum(Posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af00e89",
   "metadata": {},
   "source": [
    "### The parameter with maximum posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one sample max parameter\n",
    "max_idx = torch.argsort(Posteriors, descending=True)[0]\n",
    "\n",
    "grid_resolution = 100\n",
    "X = torch.arange(xmin, xmax, (xmax - xmin)/(grid_resolution - 0.01))\n",
    "Y = torch.arange(ymin, ymax, (ymax - ymin)/(grid_resolution - 0.01))\n",
    "\n",
    "grid_x, grid_y = torch.meshgrid(X, Y, indexing='ij')\n",
    "scan_inputs = torch.cat([grid_x.reshape([-1,1]), grid_y.reshape([-1,1])], axis=1).to(device)\n",
    "\n",
    "model = LogisticRegression(input_dim=dim, output_dim=1).to(device)\n",
    "model.linear.weight.data = ws[max_idx].reshape([1,2])\n",
    "model.linear.bias.data = -bs[max_idx]  # -b\n",
    "\n",
    "discri_vals = model(scan_inputs)\n",
    "print(discri_vals)\n",
    "discri_vals = discri_vals.reshape([grid_resolution,grid_resolution]).T\n",
    "plt.contour(X, Y, discri_vals.to('cpu').detach().numpy(), [0.5])\n",
    "plt.scatter(data1.to('cpu')[:,0], data1.to('cpu')[:,1])\n",
    "plt.scatter(data2.to('cpu')[:,0], data2.to('cpu')[:,1])\n",
    "plt.title('One sample max boundary - Approximation of MAP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all parameter samples\n",
    "\n",
    "grid_resolution = 100\n",
    "X = torch.arange(xmin, xmax, (xmax - xmin)/(grid_resolution - 0.01))\n",
    "Y = torch.arange(ymin, ymax, (ymax - ymin)/(grid_resolution - 0.01))\n",
    "\n",
    "# Produce grid data\n",
    "grid_x, grid_y = torch.meshgrid(X, Y, indexing='ij')\n",
    "scan_inputs = torch.cat([grid_x.reshape([-1,1]), grid_y.reshape([-1,1])], axis=1).to(device)\n",
    "\n",
    "High_Posterior_idxes = torch.argsort(Posteriors, descending=True)\n",
    "discri_vals = torch.zeros(grid_resolution*grid_resolution, device=device)\n",
    "discri_vals_sq = torch.zeros(grid_resolution*grid_resolution, device=device)\n",
    "print(discri_vals)\n",
    "n_sum_sample = n_param_sample\n",
    "model = LogisticRegression(input_dim=dim, output_dim=1)\n",
    "for i_param_sample in High_Posterior_idxes[0:n_sum_sample]:\n",
    "    # insert the parameter samples\n",
    "    model.linear.weight.data = ws[i_param_sample].reshape([1,2])\n",
    "    model.linear.bias.data = -bs[i_param_sample]  # -b\n",
    "\n",
    "    # Weighted averaging of the class predictions (Sum of [class posterior]*[parameter posterior])\n",
    "    discri_vals = discri_vals + model(scan_inputs).squeeze()*Posteriors[i_param_sample]\n",
    "    discri_vals_sq = discri_vals_sq + model(scan_inputs).squeeze()**2*Posteriors[i_param_sample]\n",
    "\n",
    "print(discri_vals)\n",
    "discri_vals_plot = discri_vals.reshape([grid_resolution,grid_resolution]).T\n",
    "plt.contour(X, Y, discri_vals_plot.to('cpu').detach().numpy(), [0.5])\n",
    "plt.scatter(data1.to('cpu')[:,0], data1.to('cpu')[:,1])\n",
    "plt.scatter(data2.to('cpu')[:,0], data2.to('cpu')[:,1])\n",
    "plt.title('Combine the prediction of all parameter smaples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2ed78",
   "metadata": {},
   "source": [
    "#### Print the parameters with the highest posteriors\n",
    "\n",
    "Take a look at the $\\mathbf{w}$, $b$ parameters having the highest parameter posteriors, and their posterior values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_param_sample in High_Posterior_idxes[0:3]:\n",
    "    model = LogisticRegression(input_dim=dim, output_dim=1)\n",
    "    model.linear.weight.data = ws[i_param_sample].reshape([1,2])\n",
    "    model.linear.bias.data = -bs[i_param_sample]  # -b\n",
    "\n",
    "    print(ws[i_param_sample], bs[i_param_sample], Posteriors[i_param_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fddb0a",
   "metadata": {},
   "source": [
    "### Contour plot of the variance of the prediction result and the Bayesian prediction result\n",
    "\n",
    "This is the most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "discri_var = discri_vals_sq - discri_vals**2\n",
    "discri_var[torch.where(discri_var < 0)] = 0  # small negative values\n",
    "\n",
    "discri_var_plot = discri_var.reshape([grid_resolution,grid_resolution]).T\n",
    "discri_var_plot.shape\n",
    "\n",
    "plt.contourf(X, Y, discri_var_plot.to('cpu').detach().numpy())\n",
    "plt.colorbar()\n",
    "# plt.clim(0, .2)\n",
    "plt.scatter(data1.to('cpu')[:,0], data1.to('cpu')[:,1])\n",
    "plt.scatter(data2.to('cpu')[:,0], data2.to('cpu')[:,1])\n",
    "plt.title('Variance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.contourf(X, Y, discri_vals_plot.to('cpu').detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.scatter(data1.to('cpu')[:,0], data1.to('cpu')[:,1])\n",
    "plt.scatter(data2.to('cpu')[:,0], data2.to('cpu')[:,1])\n",
    "plt.title('Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579502f7",
   "metadata": {},
   "source": [
    "### Compare with Frequentist's Logistic Regression result\n",
    "\n",
    "We also have an alternative method of using logistic regression model. We can select a single parameter $\\mathbf{w}$ and $b$ having the highest likelihood (=negative cross entropy). Compare the Bayesian result with that of Frequentist's approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc46168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Frequentist optimization\n",
    "\n",
    "losses = []\n",
    "losses_test = []\n",
    "Iterations = []\n",
    "iter = 0\n",
    "epochs = 20_000\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in tqdm(range(int(epochs)),desc='Training Epochs'):\n",
    "    x = all_data\n",
    "    labels = ys\n",
    "    optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "    outputs = model(x)\n",
    "    loss = criterion(torch.squeeze(outputs), labels) # [200,1] -squeeze-> [200]\n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "\n",
    "model_plot(model, all_data, labels,'A single optimal boundary from frequentist''s method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037c3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
